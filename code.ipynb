{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f8c893",
   "metadata": {},
   "source": [
    "### Подготовка текста для анализа\n",
    "Делаем лемматизацию с помощью библиотеки natasha, удаляем стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b0789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import MorphVocab, Doc, NewsNERTagger, NewsMorphTagger, NewsEmbedding, Segmenter\n",
    "import re, os\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9337bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('real_news.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9618dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Делаем предобработку (это может занять время)\n",
    "\n",
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01c383eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Делаем лемматизацию\n",
    "lemmas = []\n",
    "\n",
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)\n",
    "    lemmas.append(token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4d93e",
   "metadata": {},
   "source": [
    "### Убираем стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8c331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NLTK's list of english stopwords.txt\") as f:\n",
    "    stop_words = f.read()\n",
    "    \n",
    "stop_words = stop_words.split('\\n')    \n",
    "    \n",
    "clean_lemmas = []\n",
    "\n",
    "for lemma in lemmas:\n",
    "    if lemma not in stop_words and re.fullmatch(\"[\\w\\-]+\", lemma) and lemma != '-':\n",
    "        clean_lemmas.append(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9501c2",
   "metadata": {},
   "source": [
    "### Строим граф совместной встречаемости слов\n",
    "<p>Собираем пары слов и считаем для каждой пары частоту.</p>\n",
    "<p>Откуда берутся пары и что такое совместная встречаемость?\n",
    "*Это предложение для примера*. Получаем пары: Это предложение - предложение для - для примера</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de886012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1cb38a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "for i in range(len(clean_lemmas)-1):\n",
    "    #min и max нужны, чтобы слова в паре всегда стояли в алфавитном порядке\n",
    "    #это позволит считать \"это предложение\" и \"предложение это\" за одну пару\n",
    "    #т.е. совместную встречаемость слов \"это\" и \"предложение\"\n",
    "    pair = min(clean_lemmas[i], clean_lemmas[i+1]) + ',' + max(clean_lemmas[i], clean_lemmas[i+1])\n",
    "    pairs.append(pair) \n",
    "\n",
    "counts = Counter(pairs).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eff25c",
   "metadata": {},
   "source": [
    "### Фильтруем пары с низкой частотой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d44e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitered_counts = []\n",
    "\n",
    "n = 10 #частота, по которой отсекаем низкочастотные пары, можно указать другое значение\n",
    "\n",
    "for count in counts:\n",
    "    if count[-1] > n:\n",
    "        fitered_counts.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce74975",
   "metadata": {},
   "source": [
    "### Записываем результат в csv-файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee607513",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"natasha_keywords_graph_filtered.csv\"\n",
    "\n",
    "csv = 'source,target,weight' + '\\n'\n",
    "\n",
    "for count in fitered_counts:\n",
    "    line = count[0] + ',' + str(count[1]) + '\\n'\n",
    "    csv += line\n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e5295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eec0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224528cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
